<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Interactive Voice Sentiment Analyzer with Chat</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <style>
        body {
            font-family: 'Inter', sans-serif;
        }
        .custom-scrollbar::-webkit-scrollbar {
            width: 8px;
        }
        .custom-scrollbar::-webkit-scrollbar-track {
            background: #f1f1f1;
            border-radius: 10px;
        }
        .custom-scrollbar::-webkit-scrollbar-thumb {
            background: #888;
            border-radius: 10px;
        }
        .custom-scrollbar::-webkit-scrollbar-thumb:hover {
            background: #555;
        }
        #chartContainer {
            position: relative;
            height: 50vh; /* Adjusted height */
            max-height: 350px;
            width: 100%;
        }
        @media (min-width: 768px) { /* md breakpoint */
            #chartContainer {
                height: 300px; /* Or a fixed height that works with side-by-side layout */
            }
            #chatMessages {
                 height: 300px; /* Match chart height for alignment */
            }
        }
        .mic-button {
            transition: background-color 0.3s ease, transform 0.1s ease;
        }
        .mic-button:active {
            transform: scale(0.95);
        }
        .listening {
            animation: pulse 1.5s infinite;
        }
        @keyframes pulse {
            0% { box-shadow: 0 0 0 0 rgba(239, 68, 68, 0.7); } /* Tailwind red-500 */
            70% { box-shadow: 0 0 0 10px rgba(239, 68, 68, 0); }
            100% { box-shadow: 0 0 0 0 rgba(239, 68, 68, 0); }
        }

        /* Chat message styling */
        .message {
            padding: 8px 12px;
            margin-bottom: 8px;
            border-radius: 10px;
            max-width: 90%; /* Adjusted for potentially narrower chat panel */
            word-wrap: break-word;
            line-height: 1.4;
        }
        .message-user {
            background-color: #DCF8C6; 
            align-self: flex-end;
            margin-left: auto;
            color: #303030;
        }
        .message-gemini {
            background-color: #E5E7EB; 
            align-self: flex-start;
            margin-right: auto;
            color: #1F2937; 
        }
        .message-system {
            background-color: #FEF3C7; 
            color: #92400E; 
            align-self: center;
            font-style: italic;
            text-align: center;
            width: fit-content;
            margin-left: auto;
            margin-right: auto;
            font-size: 0.875rem; /* text-sm */
        }
        .message-error {
            background-color: #FEE2E2; 
            color: #B91C1C; 
            align-self: center;
            text-align: center;
            width: fit-content;
            margin-left: auto;
            margin-right: auto;
            font-size: 0.875rem; /* text-sm */
        }
        #chatMessages {
            display: flex;
            flex-direction: column;
            gap: 6px; 
        }
    </style>
</head>
<body class="bg-gray-100 min-h-screen flex flex-col items-center justify-center p-4">

    <div class="bg-white p-6 sm:p-8 rounded-xl shadow-2xl w-full max-w-4xl"> {/* Increased max-width for side-by-side */}
        <header class="mb-6 text-center">
            <h1 class="text-3xl font-bold text-gray-800">Voice Sentiment Analyzer & Chat</h1>
            <p class="text-gray-600 mt-1">Speak for sentiment analysis and an interactive voice chat with Gemini.</p>
        </header>

        <div class="mb-6 flex flex-col sm:flex-row justify-center items-center space-y-3 sm:space-y-0 sm:space-x-4">
            <button id="startListenButton" class="mic-button w-full sm:w-auto bg-green-500 hover:bg-green-600 text-white font-semibold py-3 px-6 rounded-lg shadow-md hover:shadow-lg focus:outline-none focus:ring-2 focus:ring-green-500 focus:ring-opacity-50 transform hover:scale-105">
                Start Voice Chat
            </button>
            <button id="stopListenButton" class="mic-button w-full sm:w-auto bg-red-500 hover:bg-red-600 text-white font-semibold py-3 px-6 rounded-lg shadow-md hover:shadow-lg focus:outline-none focus:ring-2 focus:ring-red-500 focus:ring-opacity-50 transform hover:scale-105" disabled>
                Stop Voice Chat
            </button>
        </div>
        
        <div class="mb-4 p-3 bg-gray-50 rounded-lg border border-gray-200 min-h-[60px]" id="transcribedTextContainer" style="display: none;"> <h3 class="text-sm font-semibold text-gray-700 mb-1">You said (live):</h3>
            <p id="transcribedTextOutput" class="text-sm text-gray-800 italic break-words">...</p>
        </div>

        <div id="statusMessage" class="mb-4 text-sm text-center text-gray-600 min-h-[20px]"></div>
        <div id="errorMessage" class="mb-4 text-sm text-center text-red-600 min-h-[20px]"></div>
        
        <div class="flex flex-col md:flex-row md:space-x-6 mb-6">
            <div id="chartContainer" class="w-full md:w-1/2 mb-6 md:mb-0">
                <canvas id="sentimentChart"></canvas>
            </div>
            
            <div class="w-full md:w-1/2">
                <h2 class="text-xl font-semibold text-gray-700 mb-3 text-center">Conversation Log</h2>
                <div id="chatMessages" class="h-64 md:h-[300px] overflow-y-auto p-3 bg-gray-50 rounded-lg border border-gray-200 custom-scrollbar">
                    <div class="message message-system">Click "Start Voice Chat" to begin.</div>
                </div>
            </div>
        </div>

        <div class="mt-8" id="rawJsonResponseContainer" style="display: none;">
            <h3 class="text-sm font-semibold text-gray-700 mb-1">Raw Sentiment API Response (JSON):</h3>
            <pre id="rawJsonResponse" class="text-xs bg-gray-800 text-green-400 p-3 rounded-lg overflow-x-auto max-h-40 custom-scrollbar"></pre>
        </div>
    </div>

    <footer class="text-center text-sm text-gray-500 mt-8 pb-4">
        Powered by Web Speech API, Gemini & Chart.js
    </footer>

    <script>
        const startListenButton = document.getElementById('startListenButton');
        const stopListenButton = document.getElementById('stopListenButton');
        const statusMessage = document.getElementById('statusMessage');
        const errorMessage = document.getElementById('errorMessage');
        const sentimentChartCanvas = document.getElementById('sentimentChart');
        
        const transcribedTextContainer = document.getElementById('transcribedTextContainer');
        const transcribedTextOutput = document.getElementById('transcribedTextOutput');
        
        const rawJsonResponseContainer = document.getElementById('rawJsonResponseContainer');
        const rawJsonResponse = document.getElementById('rawJsonResponse');

        const chatMessagesContainer = document.getElementById('chatMessages');
        // Text input elements are removed

        let sentimentChartInstance = null;
        const sentimentLabels = ['Angry', 'Calm', 'Neutral', 'Happy', 'Annoyed'];
        const sentimentColors = ['rgba(255, 99, 132, 0.7)','rgba(54, 162, 235, 0.7)','rgba(201, 203, 207, 0.7)','rgba(75, 192, 192, 0.7)','rgba(255, 159, 64, 0.7)'];
        const sentimentBorderColors = ['rgba(255, 99, 132, 1)','rgba(54, 162, 235, 1)','rgba(201, 203, 207, 1)','rgba(75, 192, 192, 1)','rgba(255, 159, 64, 1)'];

        let geminiChatHistory = []; 

        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
        let recognition;
        const speechSynthesis = window.speechSynthesis;
        let isSpeaking = false; 

        if (SpeechRecognition && speechSynthesis) {
            recognition = new SpeechRecognition();
            recognition.continuous = false; 
            recognition.interimResults = true; 
            recognition.lang = 'en-US';

            recognition.onstart = () => {
                statusMessage.textContent = 'Listening... Speak now.';
                startListenButton.disabled = true;
                stopListenButton.disabled = false;
                startListenButton.classList.add('opacity-50', 'cursor-not-allowed', 'listening');
                stopListenButton.classList.remove('opacity-50', 'cursor-not-allowed');
                errorMessage.textContent = '';
                transcribedTextContainer.style.display = 'block'; 
                transcribedTextOutput.textContent = "...";
            };

            recognition.onresult = async (event) => {
                let interimTranscript = '';
                let finalTranscript = '';

                for (let i = event.resultIndex; i < event.results.length; ++i) {
                    if (event.results[i].isFinal) {
                        finalTranscript += event.results[i][0].transcript;
                    } else {
                        interimTranscript += event.results[i][0].transcript;
                    }
                }
                transcribedTextOutput.textContent = finalTranscript + interimTranscript;

                if (finalTranscript.trim() && !isSpeaking) { 
                    recognition.stop(); 
                    
                    const spokenText = finalTranscript.trim();
                    displayMessage(spokenText, 'user');
                    statusMessage.textContent = `Recognized: "${spokenText}". Analyzing & getting Gemini's response...`;
                    
                    await getSentimentAnalysis(spokenText);
                    await getGeminiChatResponseAndSpeak(spokenText);
                }
            };

            recognition.onerror = (event) => {
                console.error('Speech recognition error:', event.error);
                let errorMsgText = `Speech recognition error: ${event.error}.`;
                if (event.error === 'no-speech') {
                    errorMsgText = "No speech detected. Please try speaking louder or closer to the microphone.";
                } else if (event.error === 'audio-capture') {
                    errorMsgText = "Audio capture error. Ensure your microphone is connected and working. Check system audio settings.";
                } else if (event.error === 'not-allowed') {
                    errorMsgText = "Microphone access denied. Please allow microphone access in your browser's site settings and refresh the page. You may need to click the lock icon near the address bar.";
                }
                
                errorMessage.textContent = errorMsgText;
                statusMessage.textContent = 'Voice chat error.';
                stopVoiceChat(); 
            };

            recognition.onend = () => {
                if (!isSpeaking && startListenButton.disabled) { 
                    if (!stopListenButton.disabled) { 
                        // Intentionally left blank for now, as continuous restart might be too aggressive.
                        // User will need to speak again for recognition to pick up if it stops due to silence.
                    }
                }
                 if (!startListenButton.disabled && !isSpeaking) { 
                    statusMessage.textContent = 'Voice chat stopped. Click "Start Voice Chat" to begin.';
                }
            };

        } else {
            statusMessage.textContent = 'Speech recognition or synthesis not supported by your browser. Please try a modern browser like Chrome or Edge.';
            startListenButton.disabled = true;
            stopListenButton.disabled = true;
            startListenButton.classList.add('opacity-50', 'cursor-not-allowed');
            startListenButton.title = "Voice features not supported by your browser.";
        }
        
        function startVoiceChat() {
            if (recognition && speechSynthesis) {
                geminiChatHistory = []; 
                const initialSystemMessage = chatMessagesContainer.querySelector('.message-system');
                if (initialSystemMessage && initialSystemMessage.textContent.includes('Click "Start Voice Chat" to begin.')) {
                    initialSystemMessage.remove(); // Remove initial prompt if it exists
                }
                displayMessage("Voice chat started. Gemini is ready.", "system");
                transcribedTextOutput.textContent = "..."; 
                errorMessage.textContent = ""; 
                statusMessage.textContent = "Initializing voice chat... Please allow microphone access if prompted.";
                try {
                    recognition.start();
                } catch (e) {
                    console.warn("Recognition error starting:", e);
                     if (e.name === 'InvalidStateError') { 
                        statusMessage.textContent = 'Voice chat active. Speak now.';
                        startListenButton.disabled = true;
                        stopListenButton.disabled = false;
                        startListenButton.classList.add('listening');
                     } else {
                        errorMessage.textContent = "Could not start voice chat. Check console and microphone permissions.";
                     }
                }
            }
        }

        function stopVoiceChat() {
            if (recognition) recognition.stop();
            if (speechSynthesis.speaking) speechSynthesis.cancel(); 
            isSpeaking = false;

            startListenButton.disabled = false;
            stopListenButton.disabled = true;
            startListenButton.classList.remove('opacity-50', 'cursor-not-allowed', 'listening');
            stopListenButton.classList.add('opacity-50', 'cursor-not-allowed');
            statusMessage.textContent = 'Voice chat stopped.';
            transcribedTextContainer.style.display = 'none';
        }

        startListenButton.addEventListener('click', startVoiceChat);
        stopListenButton.addEventListener('click', stopVoiceChat);

        function speakText(textToSpeak) {
            return new Promise((resolve, reject) => {
                if (!speechSynthesis) {
                    errorMessage.textContent = "Text-to-speech not available.";
                    return reject("TTS not available");
                }
                if (speechSynthesis.speaking) { 
                    speechSynthesis.cancel();
                }

                const utterance = new SpeechSynthesisUtterance(textToSpeak);
                utterance.lang = 'en-US'; 
                
                isSpeaking = true;
                utterance.onstart = () => {
                    statusMessage.textContent = "Gemini is speaking...";
                    if (recognition) recognition.stop(); 
                };
                utterance.onend = () => {
                    isSpeaking = false;
                    statusMessage.textContent = "Finished speaking. Listening for your reply...";
                    if (startListenButton.disabled && recognition) { 
                        try {
                           recognition.start(); 
                        } catch(e) {
                            console.error("Error restarting recognition after TTS:", e);
                            errorMessage.textContent = "Error restarting listener. Please try stopping and starting voice chat.";
                            stopVoiceChat();
                        }
                    }
                    resolve();
                };
                utterance.onerror = (event) => {
                    isSpeaking = false;
                    console.error('Speech synthesis error:', event.error);
                    errorMessage.textContent = `Error speaking: ${event.error}.`;
                     if (startListenButton.disabled && recognition) {
                        statusMessage.textContent = "Error speaking. Listening for your reply...";
                         try {
                            recognition.start(); 
                         } catch(e) {
                             console.error("Error restarting recognition after TTS error:", e);
                             errorMessage.textContent = "Error restarting listener. Please try stopping and starting voice chat.";
                             stopVoiceChat();
                         }
                    }
                    reject(event.error);
                };
                speechSynthesis.speak(utterance);
            });
        }

        function initializeChart() {
            if (sentimentChartInstance) sentimentChartInstance.destroy();
            const ctx = sentimentChartCanvas.getContext('2d');
            sentimentChartInstance = new Chart(ctx, { 
                type: 'bar', data: { labels: sentimentLabels, datasets: [{ label: 'Sentiment Score', data: [0,0,0,0,0], backgroundColor: sentimentColors, borderColor: sentimentBorderColors, borderWidth: 1 }] },
                options: { responsive: true, maintainAspectRatio: false, scales: { y: { beginAtZero: true, max: 1.0, title: { display: true, text: 'Sentiment Score (0.0 - 1.0)', font: {size: 14}}, ticks: {stepSize: 0.1}}, x: {ticks: {font: {size: 12}}}}, plugins: { legend: {display: false}, title: {display: true, text: 'Sentiment Analysis (Last Input)', font: {size: 16, weight: 'bold'}, padding: {top: 10, bottom: 20}}, tooltip: {callbacks: {label: function(context){let label=context.dataset.label||''; if(label){label+=': ';} if(context.parsed.y!==null){label+=context.parsed.y.toFixed(2);} return label;}}}}}
            });
        }
        function updateChart(scores) {
            if (sentimentChartInstance) {
                sentimentChartInstance.data.datasets[0].data = [scores.angry||0, scores.calm||0, scores.neutral||0, scores.happy||0, scores.annoyed||0];
                sentimentChartInstance.update();
            }
        }

        async function getSentimentAnalysis(text) {
            errorMessage.textContent = '';
            const apiKey = "AIzaSyBpNS9buNt6TJOsWgr_zK0LtPW_Jx344-E"; 
            const apiUrl = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=${apiKey}`;
            const prompt = `Analyze the sentiment of the following text. Provide a score from 0.0 to 1.0 for: Angry, Calm, Neutral, Happy, Annoyed. Text: "${text}". Return ONLY JSON.`;
            const schema = { type: "OBJECT", properties: { "angry": {"type": "NUMBER"}, "calm": {"type": "NUMBER"}, "neutral": {"type": "NUMBER"}, "happy": {"type": "NUMBER"}, "annoyed": {"type": "NUMBER"}}, required: ["angry", "calm", "neutral", "happy", "annoyed"]};
            const payload = { contents: [{ role: "user", parts: [{ text: prompt }] }], generationConfig: { responseMimeType: "application/json", responseSchema: schema, temperature: 0.2 }};

            try {
                const response = await fetch(apiUrl, { method: 'POST', headers: { 'Content-Type': 'application/json' }, body: JSON.stringify(payload) });
                if (!response.ok) { 
                    const errBody = await response.text(); 
                    console.error("Sentiment API full error:", errBody); 
                    let detailedError = `Sentiment API Error ${response.status}.`;
                    if (response.status === 403) {
                        detailedError += " This might be due to an invalid API key, missing permissions for the model, or billing issues. Please check your Google Cloud project configuration.";
                    }
                    throw new Error(detailedError); 
                }
                const result = await response.json();
                if (result.candidates && result.candidates[0].content && result.candidates[0].content.parts[0]) {
                    const rawJsonText = result.candidates[0].content.parts[0].text;
                    rawJsonResponse.textContent = JSON.stringify(JSON.parse(rawJsonText), null, 2); 
                    rawJsonResponseContainer.style.display = 'block';
                    const sentimentScores = JSON.parse(rawJsonText);
                    for (const key in sentimentScores) { if (typeof sentimentScores[key] !== 'number') sentimentScores[key] = 0; sentimentScores[key] = Math.max(0, Math.min(1, sentimentScores[key]));}
                    updateChart(sentimentScores);
                } else { throw new Error('Unexpected sentiment API response structure.'); }
            } catch (error) {
                console.error('Error during sentiment analysis:', error);
                errorMessage.textContent = error.message; 
                rawJsonResponseContainer.style.display = 'none';
            }
        }

        function displayMessage(text, sender) {
            const initialSystemMessage = chatMessagesContainer.querySelector('.message-system');
            if (initialSystemMessage && (initialSystemMessage.textContent.includes("Click \"Start Voice Chat\" to begin.") || initialSystemMessage.textContent.includes("Voice chat started."))) {
                // Allow "Voice chat started." to persist if it's the one being added.
                if (!(sender === 'system' && text.includes("Voice chat started."))) {
                     initialSystemMessage.remove();
                }
            }


            const messageElement = document.createElement('div');
            messageElement.classList.add('message', `message-${sender}`);
            messageElement.textContent = text;
            chatMessagesContainer.appendChild(messageElement);
            chatMessagesContainer.scrollTop = chatMessagesContainer.scrollHeight;
        }

        async function getGeminiChatResponseAndSpeak(userText) {
            geminiChatHistory.push({ role: "user", parts: [{ text: userText }] });
            
            // Text input is already removed, so no need to disable it.

            const apiKey = "AIzaSyBpNS9buNt6TJOsWgr_zK0LtPW_Jx344-E";
            const apiUrl = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=${apiKey}`;
            const payload = { contents: geminiChatHistory, generationConfig: { temperature: 0.7 } };

            try {
                const response = await fetch(apiUrl, { method: 'POST', headers: { 'Content-Type': 'application/json' }, body: JSON.stringify(payload) });
                if (!response.ok) { 
                    const errBody = await response.text(); 
                    console.error("Chat API full error:", errBody); 
                    let detailedError = `Chat API Error ${response.status}.`;
                    if (response.status === 403) {
                        detailedError += " This might be due to an invalid API key, missing permissions for the model, or billing issues. Please check your Google Cloud project configuration.";
                    }
                    throw new Error(detailedError);
                }
                const result = await response.json();

                if (result.candidates && result.candidates[0].content && result.candidates[0].content.parts[0]) {
                    const geminiResponseText = result.candidates[0].content.parts[0].text;
                    geminiChatHistory.push({ role: "model", parts: [{ text: geminiResponseText }] });
                    displayMessage(geminiResponseText, 'gemini');
                    await speakText(geminiResponseText); 
                } else {
                    throw new Error('Unexpected Gemini chat API response structure.');
                }
            } catch (error) {
                console.error('Error getting/speaking Gemini response:', error);
                const errText = error.message; 
                displayMessage(errText, 'error');
                errorMessage.textContent = errText;
                if (startListenButton.disabled && !isSpeaking && recognition) { 
                    statusMessage.textContent = "Error in Gemini's response. Listening for your reply...";
                     try { recognition.start(); } catch(e) { console.error("Error restarting recognition after chat API error:", e); stopVoiceChat(); }
                }
            } 
            // No finally block needed to re-enable text input as it's removed.
        }
        
        // getGeminiChatResponseForText function is removed as text input is removed.
        // Event listeners for chatInput and sendChatMessageButton are removed.


        window.onload = () => {
            initializeChart();
            stopListenButton.disabled = true;
            stopListenButton.classList.add('opacity-50', 'cursor-not-allowed');
            if (!SpeechRecognition || !speechSynthesis) {
                 startListenButton.classList.add('opacity-50', 'cursor-not-allowed');
                 startListenButton.title = "Voice features not supported by your browser.";
                 statusMessage.textContent = 'Speech recognition or synthesis not supported by your browser. Please try a modern browser like Chrome or Edge.';
            }
        };
    </script>
</body>
</html>
