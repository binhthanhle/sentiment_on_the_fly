<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Interactive Voice Sentiment Analyzer with Chat</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <style>
        body {
            font-family: 'Inter', sans-serif;
        }
        .custom-scrollbar::-webkit-scrollbar {
            width: 8px;
        }
        .custom-scrollbar::-webkit-scrollbar-track {
            background: #f1f1f1;
            border-radius: 10px;
        }
        .custom-scrollbar::-webkit-scrollbar-thumb {
            background: #888;
            border-radius: 10px;
        }
        .custom-scrollbar::-webkit-scrollbar-thumb:hover {
            background: #555;
        }
        #chartContainer {
            position: relative;
            height: 50vh; /* Adjusted height */
            max-height: 350px;
            width: 100%;
        }
        @media (min-width: 768px) { /* md breakpoint */
            #chartContainer {
                height: 300px; /* Or a fixed height that works with side-by-side layout */
            }
            #chatMessages {
                 height: 300px; /* Match chart height for alignment */
            }
        }
        .mic-button {
            transition: background-color 0.3s ease, transform 0.1s ease;
        }
        .mic-button:active {
            transform: scale(0.95);
        }
        .listening {
            animation: pulse 1.5s infinite;
        }
        @keyframes pulse {
            0% { box-shadow: 0 0 0 0 rgba(239, 68, 68, 0.7); } /* Tailwind red-500 */
            70% { box-shadow: 0 0 0 10px rgba(239, 68, 68, 0); }
            100% { box-shadow: 0 0 0 0 rgba(239, 68, 68, 0); }
        }

        /* Chat message styling */
        .message {
            padding: 8px 12px;
            margin-bottom: 8px;
            border-radius: 10px;
            max-width: 90%; /* Adjusted for potentially narrower chat panel */
            word-wrap: break-word;
            line-height: 1.4;
        }
        .message-user {
            background-color: #DCF8C6; 
            align-self: flex-end;
            margin-left: auto;
            color: #303030;
        }
        .message-gemini {
            background-color: #E5E7EB; 
            align-self: flex-start;
            margin-right: auto;
            color: #1F2937; 
        }
        .message-system {
            background-color: #FEF3C7; 
            color: #92400E; 
            align-self: center;
            font-style: italic;
            text-align: center;
            width: fit-content;
            margin-left: auto;
            margin-right: auto;
            font-size: 0.875rem; /* text-sm */
        }
        .message-error {
            background-color: #FEE2E2; 
            color: #B91C1C; 
            align-self: center;
            text-align: center;
            width: fit-content;
            margin-left: auto;
            margin-right: auto;
            font-size: 0.875rem; /* text-sm */
        }
        #chatMessages {
            display: flex;
            flex-direction: column;
            gap: 6px; 
        }
    </style>
</head>
<body class="bg-gray-100 min-h-screen flex flex-col items-center justify-center p-4">

    <div class="bg-white p-6 sm:p-8 rounded-xl shadow-2xl w-full max-w-4xl"> 
        <header class="mb-6 text-center">
            <h1 class="text-3xl font-bold text-gray-800">Voice Sentiment Analyzer & Chat</h1>
            <p class="text-gray-600 mt-1">Speak for sentiment analysis and an interactive voice chat with Gemini.</p>
        </header>

        <div class="mb-6 flex flex-col sm:flex-row justify-center items-center space-y-3 sm:space-y-0 sm:space-x-4">
            <button id="startListenButton" class="mic-button w-full sm:w-auto bg-green-500 hover:bg-green-600 text-white font-semibold py-3 px-6 rounded-lg shadow-md hover:shadow-lg focus:outline-none focus:ring-2 focus:ring-green-500 focus:ring-opacity-50 transform hover:scale-105">
                Start Voice Chat
            </button>
            <button id="stopListenButton" class="mic-button w-full sm:w-auto bg-red-500 hover:bg-red-600 text-white font-semibold py-3 px-6 rounded-lg shadow-md hover:shadow-lg focus:outline-none focus:ring-2 focus:ring-red-500 focus:ring-opacity-50 transform hover:scale-105" disabled>
                Stop Voice Chat
            </button>
        </div>
        
        <div class="mb-4 p-3 bg-gray-50 rounded-lg border border-gray-200 min-h-[60px]" id="transcribedTextContainer" style="display: none;"> <h3 class="text-sm font-semibold text-gray-700 mb-1">You said (live):</h3>
            <p id="transcribedTextOutput" class="text-sm text-gray-800 italic break-words">...</p>
        </div>

        <div id="statusMessage" class="mb-4 text-sm text-center text-gray-600 min-h-[20px]"></div>
        <div id="errorMessage" class="mb-4 text-sm text-center text-red-600 min-h-[20px]"></div>
        
        <div class="flex flex-col md:flex-row md:space-x-6 mb-6">
            <div id="chartContainer" class="w-full md:w-1/2 mb-6 md:mb-0">
                <canvas id="sentimentChart"></canvas>
            </div>
            
            <div class="w-full md:w-1/2">
                <h2 class="text-xl font-semibold text-gray-700 mb-3 text-center">Conversation Log</h2>
                <div id="chatMessages" class="h-64 md:h-[300px] overflow-y-auto p-3 bg-gray-50 rounded-lg border border-gray-200 custom-scrollbar">
                    {/* Initial message will be added by JS if chat is empty */}
                </div>
            </div>
        </div>

        <div class="mt-8" id="rawJsonResponseContainer" style="display: none;">
            <h3 class="text-sm font-semibold text-gray-700 mb-1">Raw Sentiment API Response (JSON):</h3>
            <pre id="rawJsonResponse" class="text-xs bg-gray-800 text-green-400 p-3 rounded-lg overflow-x-auto max-h-40 custom-scrollbar"></pre>
        </div>
    </div>

    <footer class="text-center text-sm text-gray-500 mt-8 pb-4">
        Powered by Web Speech API, Gemini & Chart.js
    </footer>

    <script>
        const startListenButton = document.getElementById('startListenButton');
        const stopListenButton = document.getElementById('stopListenButton');
        const statusMessage = document.getElementById('statusMessage');
        const errorMessage = document.getElementById('errorMessage');
        const sentimentChartCanvas = document.getElementById('sentimentChart');
        
        const transcribedTextContainer = document.getElementById('transcribedTextContainer');
        const transcribedTextOutput = document.getElementById('transcribedTextOutput');
        
        const rawJsonResponseContainer = document.getElementById('rawJsonResponseContainer');
        const rawJsonResponse = document.getElementById('rawJsonResponse');

        const chatMessagesContainer = document.getElementById('chatMessages');
        // Text input elements are removed

        let sentimentChartInstance = null;
        const sentimentLabels = ['Angry', 'Calm', 'Neutral', 'Happy', 'Annoyed'];
        const sentimentColors = ['rgba(255, 99, 132, 0.7)','rgba(54, 162, 235, 0.7)','rgba(201, 203, 207, 0.7)','rgba(75, 192, 192, 0.7)','rgba(255, 159, 64, 0.7)'];
        const sentimentBorderColors = ['rgba(255, 99, 132, 1)','rgba(54, 162, 235, 1)','rgba(201, 203, 207, 1)','rgba(75, 192, 192, 1)','rgba(255, 159, 64, 1)'];

        let geminiChatHistory = []; 

        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
        let recognition;
        const browserSupportsTTS = !!window.speechSynthesis; 
        let isSpeaking = false; 
        let currentAudio = null; // To control the Google TTS audio element

        if (SpeechRecognition && browserSupportsTTS) { 
            recognition = new SpeechRecognition();
            recognition.continuous = false; 
            recognition.interimResults = true; 
            recognition.lang = 'en-US';

            recognition.onstart = () => {
                statusMessage.textContent = 'Listening... Speak now.';
                startListenButton.disabled = true;
                stopListenButton.disabled = false;
                startListenButton.classList.add('opacity-50', 'cursor-not-allowed', 'listening');
                stopListenButton.classList.remove('opacity-50', 'cursor-not-allowed');
                errorMessage.textContent = '';
                transcribedTextContainer.style.display = 'block'; 
                transcribedTextOutput.textContent = "...";
            };

            recognition.onresult = async (event) => {
                let interimTranscript = '';
                let finalTranscript = '';

                for (let i = event.resultIndex; i < event.results.length; ++i) {
                    if (event.results[i].isFinal) {
                        finalTranscript += event.results[i][0].transcript;
                    } else {
                        interimTranscript += event.results[i][0].transcript;
                    }
                }
                transcribedTextOutput.textContent = finalTranscript + interimTranscript;

                if (finalTranscript.trim() && !isSpeaking) { 
                    recognition.stop(); 
                    
                    const spokenText = finalTranscript.trim();
                    displayMessage(spokenText, 'user');
                    statusMessage.textContent = `Recognized: "${spokenText}". Analyzing & getting Gemini's response...`;
                    
                    await getSentimentAnalysis(spokenText);
                    await getGeminiChatResponseAndSpeak(spokenText);
                }
            };

            recognition.onerror = (event) => {
                console.error('Speech recognition error:', event.error);
                let errorMsgText = `Speech recognition error: ${event.error}.`;
                if (event.error === 'no-speech') {
                    errorMsgText = "No speech detected. Please try speaking louder or closer to the microphone.";
                } else if (event.error === 'audio-capture') {
                    errorMsgText = "Audio capture error. Ensure your microphone is connected and working. Check system audio settings.";
                } else if (event.error === 'not-allowed') {
                    errorMsgText = "Microphone access denied. Please allow microphone access in your browser's site settings and refresh the page. You may need to click the lock icon near the address bar.";
                }
                
                errorMessage.textContent = errorMsgText;
                statusMessage.textContent = 'Voice chat error.';
                stopVoiceChat(); 
            };

            recognition.onend = () => {
                // The Web Speech API's behavior regarding silence detection and timeout before 'onend' 
                // or 'onerror' (with 'no-speech') is browser-dependent and not precisely configurable 
                // to a fixed duration like 1 second.
                // This handler primarily manages UI state when recognition stops.
                if (!isSpeaking && startListenButton.disabled) { 
                    // If recognition stopped unexpectedly (e.g., prolonged silence) while voice chat is active,
                    // and it's not because Gemini is speaking, the user will need to speak again or
                    // the system will wait for the next explicit "speak" action.
                    // Aggressive auto-restarting can be disruptive.
                    // Current logic: recognition is restarted after Gemini finishes speaking (in speakText's onended).
                }
                 if (!startListenButton.disabled && !isSpeaking) { 
                    statusMessage.textContent = 'Voice chat stopped. Click "Start Voice Chat" to begin.';
                }
            };

        } else {
            statusMessage.textContent = 'Speech recognition or synthesis not supported by your browser. Please try a modern browser like Chrome or Edge.';
            startListenButton.disabled = true;
            stopListenButton.disabled = true;
            startListenButton.classList.add('opacity-50', 'cursor-not-allowed');
            startListenButton.title = "Voice features not supported by your browser.";
        }
        
        function startVoiceChat() {
            if (recognition && browserSupportsTTS) {
                geminiChatHistory = []; 
                chatMessagesContainer.innerHTML = ''; // Clear previous chat messages from display
                displayMessage("Voice chat started. Gemini is ready.", "system");
                transcribedTextOutput.textContent = "..."; 
                errorMessage.textContent = ""; 
                statusMessage.textContent = "Initializing voice chat... Please allow microphone access if prompted.";
                try {
                    recognition.start();
                } catch (e) {
                    console.warn("Recognition error starting:", e);
                     if (e.name === 'InvalidStateError') { 
                        statusMessage.textContent = 'Voice chat active. Speak now.';
                        startListenButton.disabled = true;
                        stopListenButton.disabled = false;
                        startListenButton.classList.add('listening');
                     } else {
                        errorMessage.textContent = "Could not start voice chat. Check console and microphone permissions.";
                     }
                }
            }
        }

        function stopVoiceChat() {
            if (recognition) recognition.stop();
            if (currentAudio) { 
                currentAudio.pause();
                currentAudio.currentTime = 0;
                currentAudio = null;
            }
            isSpeaking = false;

            startListenButton.disabled = false;
            stopListenButton.disabled = true;
            startListenButton.classList.remove('opacity-50', 'cursor-not-allowed', 'listening');
            stopListenButton.classList.add('opacity-50', 'cursor-not-allowed');
            statusMessage.textContent = 'Voice chat stopped.';
            transcribedTextContainer.style.display = 'none';
            // Add the initial prompt back if the chat is empty after stopping.
            if (chatMessagesContainer.children.length === 0) {
                 displayMessage('Click "Start Voice Chat" to begin.', "system");
            }
        }

        startListenButton.addEventListener('click', startVoiceChat);
        stopListenButton.addEventListener('click', stopVoiceChat);

        async function speakText(textToSpeak) {
            return new Promise(async (resolve, reject) => {
                if (currentAudio) { 
                    currentAudio.pause();
                    currentAudio.currentTime = 0;
                }

                isSpeaking = true;
                statusMessage.textContent = "Gemini is preparing to speak...";
                if (recognition) recognition.stop(); 

                const ttsApiKey = "AIzaSyConUa08TTGzm0HCK61gtRt25MR1TDPbw4"; 
                const ttsApiUrl = `https://texttospeech.googleapis.com/v1/text:synthesize?key=${ttsApiKey}`;
                const requestBody = {
                    input: { text: textToSpeak },
                    voice: { languageCode: 'en-US', name: 'en-US-Chirp3-HD-Zubenelgenubi' }, 
                    audioConfig: { audioEncoding: 'MP3' }
                };

                try {
                    const response = await fetch(ttsApiUrl, {
                        method: 'POST',
                        headers: { 'Content-Type': 'application/json' },
                        body: JSON.stringify(requestBody)
                    });

                    if (!response.ok) {
                        const errorBody = await response.json(); 
                        console.error('Google TTS API Error Response:', errorBody);
                        let detailedError = `Google TTS API Error ${response.status}.`;
                        if (errorBody.error && errorBody.error.message) {
                             detailedError += ` ${errorBody.error.message}`;
                        }
                        if (response.status === 403) {
                             detailedError += " Check API key, TTS API enablement, and billing.";
                        }
                        throw new Error(detailedError);
                    }

                    const result = await response.json();
                    if (!result.audioContent) {
                        throw new Error("No audio content received from Google TTS API.");
                    }

                    currentAudio = new Audio(`data:audio/mp3;base64,${result.audioContent}`);
                    
                    currentAudio.oncanplaythrough = () => {
                         statusMessage.textContent = "Gemini is speaking...";
                    }
                    currentAudio.play();

                    currentAudio.onended = () => {
                        isSpeaking = false;
                        currentAudio = null;
                        statusMessage.textContent = "Finished speaking. Listening for your reply...";
                        // The transition to listening again happens quickly after TTS ends.
                        // Precise 1-second control is not feasible with current Web APIs.
                        if (startListenButton.disabled && recognition) { 
                            try {
                               recognition.start(); 
                            } catch(e) {
                                console.error("Error restarting recognition after TTS:", e);
                                errorMessage.textContent = "Error restarting listener. Please try stopping and starting voice chat.";
                                stopVoiceChat();
                            }
                        }
                        resolve();
                    };

                    currentAudio.onerror = (e) => {
                        isSpeaking = false;
                        currentAudio = null;
                        console.error('Audio playback error:', e);
                        errorMessage.textContent = 'Error playing Gemini\'s voice.';
                        if (startListenButton.disabled && recognition) {
                            statusMessage.textContent = "Error playing voice. Listening for your reply...";
                            try {
                                recognition.start(); 
                            } catch(err) {
                                 console.error("Error restarting recognition after audio playback error:", err);
                                 stopVoiceChat();
                            }
                        }
                        reject('Audio playback error');
                    };

                } catch (error) {
                    isSpeaking = false;
                    console.error('Google TTS API request or processing error:', error);
                    errorMessage.textContent = `TTS Error: ${error.message.substring(0, 150)}`;
                    if (startListenButton.disabled && recognition) {
                        statusMessage.textContent = "Error with Gemini's voice. Listening for your reply...";
                        try {
                            recognition.start();
                        } catch(e) {
                            console.error("Error restarting recognition after TTS API error:", e);
                            stopVoiceChat();
                        }
                    }
                    reject(error.message);
                }
            });
        }


        function initializeChart() {
            if (sentimentChartInstance) sentimentChartInstance.destroy();
            const ctx = sentimentChartCanvas.getContext('2d');
            sentimentChartInstance = new Chart(ctx, { 
                type: 'bar', data: { labels: sentimentLabels, datasets: [{ label: 'Sentiment Score', data: [0,0,0,0,0], backgroundColor: sentimentColors, borderColor: sentimentBorderColors, borderWidth: 1 }] },
                options: { responsive: true, maintainAspectRatio: false, scales: { y: { beginAtZero: true, max: 1.0, title: { display: true, text: 'Sentiment Score (0.0 - 1.0)', font: {size: 14}}, ticks: {stepSize: 0.1}}, x: {ticks: {font: {size: 12}}}}, plugins: { legend: {display: false}, title: {display: true, text: 'Sentiment Analysis (Last Input)', font: {size: 16, weight: 'bold'}, padding: {top: 10, bottom: 20}}, tooltip: {callbacks: {label: function(context){let label=context.dataset.label||''; if(label){label+=': ';} if(context.parsed.y!==null){label+=context.parsed.y.toFixed(2);} return label;}}}}}
            });
        }
        function updateChart(scores) {
            if (sentimentChartInstance) {
                sentimentChartInstance.data.datasets[0].data = [scores.angry||0, scores.calm||0, scores.neutral||0, scores.happy||0, scores.annoyed||0];
                sentimentChartInstance.update();
            }
        }

        async function getSentimentAnalysis(text) {
            errorMessage.textContent = '';
            const apiKey = "AIzaSyConUa08TTGzm0HCK61gtRt25MR1TDPbw4"; 
            const apiUrl = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=${apiKey}`;
            const prompt = `Analyze the sentiment of the following text. Provide a score from 0.0 to 1.0 for: Angry, Calm, Neutral, Happy, Annoyed. Text: "${text}". Return ONLY JSON.`;
            const schema = { type: "OBJECT", properties: { "angry": {"type": "NUMBER"}, "calm": {"type": "NUMBER"}, "neutral": {"type": "NUMBER"}, "happy": {"type": "NUMBER"}, "annoyed": {"type": "NUMBER"}}, required: ["angry", "calm", "neutral", "happy", "annoyed"]};
            const payload = { contents: [{ role: "user", parts: [{ text: prompt }] }], generationConfig: { responseMimeType: "application/json", responseSchema: schema, temperature: 0.2 }};

            try {
                const response = await fetch(apiUrl, { method: 'POST', headers: { 'Content-Type': 'application/json' }, body: JSON.stringify(payload) });
                if (!response.ok) { 
                    const errBody = await response.text(); 
                    console.error("Sentiment API full error:", errBody); 
                    let detailedError = `Sentiment API Error ${response.status}.`;
                    if (response.status === 403) {
                        detailedError += " This might be due to an invalid API key, missing permissions for the model, or billing issues. Please check your Google Cloud project configuration.";
                    }
                    throw new Error(detailedError); 
                }
                const result = await response.json();
                if (result.candidates && result.candidates[0].content && result.candidates[0].content.parts[0]) {
                    const rawJsonText = result.candidates[0].content.parts[0].text;
                    rawJsonResponse.textContent = JSON.stringify(JSON.parse(rawJsonText), null, 2); 
                    rawJsonResponseContainer.style.display = 'block';
                    const sentimentScores = JSON.parse(rawJsonText);
                    for (const key in sentimentScores) { if (typeof sentimentScores[key] !== 'number') sentimentScores[key] = 0; sentimentScores[key] = Math.max(0, Math.min(1, sentimentScores[key]));}
                    updateChart(sentimentScores);
                } else { throw new Error('Unexpected sentiment API response structure.'); }
            } catch (error) {
                console.error('Error during sentiment analysis:', error);
                errorMessage.textContent = error.message; 
                rawJsonResponseContainer.style.display = 'none';
            }
        }

        function displayMessage(text, sender) {
            // Remove initial "Click Start Voice Chat" message if it exists and we are adding any other message.
            const initialSystemMessage = chatMessagesContainer.querySelector('.message-system');
            if (initialSystemMessage && initialSystemMessage.textContent.includes("Click \"Start Voice Chat\" to begin.")) {
                 initialSystemMessage.remove();
            }

            const messageElement = document.createElement('div');
            messageElement.classList.add('message', `message-${sender}`);
            messageElement.textContent = text;
            chatMessagesContainer.appendChild(messageElement);
            chatMessagesContainer.scrollTop = chatMessagesContainer.scrollHeight;
        }

        async function getGeminiChatResponseAndSpeak(userText) {
            geminiChatHistory.push({ role: "user", parts: [{ text: userText }] });
            
            const apiKey = "AIzaSyConUa08TTGzm0HCK61gtRt25MR1TDPbw4";
            const apiUrl = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=${apiKey}`;
            const payload = { contents: geminiChatHistory, generationConfig: { temperature: 0.7 } };

            try {
                const response = await fetch(apiUrl, { method: 'POST', headers: { 'Content-Type': 'application/json' }, body: JSON.stringify(payload) });
                if (!response.ok) { 
                    const errBody = await response.text(); 
                    console.error("Chat API full error:", errBody); 
                    let detailedError = `Chat API Error ${response.status}.`;
                    if (response.status === 403) {
                        detailedError += " This might be due to an invalid API key, missing permissions for the model, or billing issues. Please check your Google Cloud project configuration.";
                    }
                    throw new Error(detailedError);
                }
                const result = await response.json();

                if (result.candidates && result.candidates[0].content && result.candidates[0].content.parts[0]) {
                    const geminiResponseText = result.candidates[0].content.parts[0].text;
                    geminiChatHistory.push({ role: "model", parts: [{ text: geminiResponseText }] });
                    displayMessage(geminiResponseText, 'gemini');
                    await speakText(geminiResponseText); 
                } else {
                    throw new Error('Unexpected Gemini chat API response structure.');
                }
            } catch (error) {
                console.error('Error getting/speaking Gemini response:', error);
                const errText = error.message; 
                displayMessage(errText, 'error');
                errorMessage.textContent = errText;
                if (startListenButton.disabled && !isSpeaking && recognition) { 
                    statusMessage.textContent = "Error in Gemini's response. Listening for your reply...";
                     try { recognition.start(); } catch(e) { console.error("Error restarting recognition after chat API error:", e); stopVoiceChat(); }
                }
            } 
        }
        
        window.onload = () => {
            initializeChart();
            stopListenButton.disabled = true;
            stopListenButton.classList.add('opacity-50', 'cursor-not-allowed');
            // Add initial message to chat log on load
            if (chatMessagesContainer.children.length === 0) {
                 displayMessage('Click "Start Voice Chat" to begin.', "system");
            }

            if (!SpeechRecognition || !browserSupportsTTS) { 
                 startListenButton.classList.add('opacity-50', 'cursor-not-allowed');
                 startListenButton.title = "Voice features not supported by your browser.";
                 statusMessage.textContent = 'Speech recognition or synthesis not supported by your browser. Please try a modern browser like Chrome or Edge.';
            }
        };
    </script>
</body>
</html>
